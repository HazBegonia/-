# 项目过程详细

### git上传

在 GitHub 端创建个新的仓库

[Github](https://github.com/new)，创建成功后获得 SSH 地址 `https://github.com/username/my-project.git`

Github 使用：

本地和 Github 链接：

- 第一步：获取你的公钥内容：`notepad %userprofile%\.ssh\id_rsa.pub` 全选复制
- 第二步：告诉 GitHub 你的公钥：头像 -> Settings -> 在左侧菜单栏找到并点击 SSH and GPG keys -> 点击绿色的 New SSH key 按钮 -> Key：把刚才从记事本里复制的内容，粘贴进去 -> 保存
- 第三步：验证并再次推送：测试连接 `ssh -T git@github.com`

具体操作

- 初始化 Git 仓库： `git init`
- 将文件添加到暂存区： `git add .`
- 提交到本地仓库： `git commit -m "update"`



### 环境配置

python环境采用 conda 

创建环境： `conda create --name books python=3.11`

使用环境： `conda activate books`

停用环境： `conda deactivate`



### 第一阶段：数据采集与持久化 (Scrapy + MySQL) 已完成✅️

先安装scrapy和pymysql。

创建 ：

```
scrapy startproject Books_Obtain
cd Books_Obtain
scrapy genspider books https://books.toscrape.com/
```

设置 ipython：

在 scrapy.org 中的 settings 里加入 `shell = ipython`

然后每次进入输入 scrapy shell，再 `fetch('url')`

过程：

所有的信息都可以在里面的标签爬出，因此只需先进入链接，直接爬取里面的即可。

过程：

在spiders的books.py里面写解析和爬取，在items.py里要写爬取信息，无需登录无需验证不用中间件。

钩子函数（start_requests）里爬取每一页 -> 回调钩子函数（parse）-> 获取每一页中每一本书的链接进入（parse_detail）

爬取具体信息方法，[链接](https://docs.scrapy.org/en/latest/intro/tutorial.html)，用 CSS 或者 XPath，处理完了装入 Mysql

设置延迟时间为2s，实际上，爬取1000本书花了接近50min



### 第二阶段：后端架构与 API 开发 (Django)

先创建好项目

```
django-admin startproject Books_web
cd Books_web
python3 manage.py startapp users
vue create my-book-frontend
cd my-book-frontend
npm install jquery
```

登录模块

Vue代码来自[登录](https://github.com/xlxDH/King-of-Bots/blob/main/web/src/views/user/account/UserAccountIndex.vue)，

主要问题：

![image-20260212132722325](C:\Users\85228\AppData\Roaming\Typora\typora-user-images\image-20260212132722325.png)

搜索逻辑：

采用 Trie 树，由于数据量不多，所以先事先存入所有数据到 Trie 树中，注意存入的信息，其实只要存入upc（唯一标识）就好了，并不用存入全部。为了方便数据的可观性，我这里又存了书名这个信息。然后为了防止信息爆炸，对数量设置了一个阈值为20。当用户查询时，先从 Trie 中取出若干的 upc，再从数据库中读取 upc 对应的书本信息。

### 第三阶段：机器学习推荐系统 (推荐逻辑)

### 第四阶段：前端界面开发 (Vue3)

### 第五阶段：项目集成与调试

